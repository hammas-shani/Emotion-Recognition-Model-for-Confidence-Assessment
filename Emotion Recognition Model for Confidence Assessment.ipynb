{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":234911,"sourceType":"datasetVersion","datasetId":99505},{"sourceId":5536790,"sourceType":"datasetVersion","datasetId":3191137},{"sourceId":12734057,"sourceType":"datasetVersion","datasetId":8049144}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Define paths\ninput_base_path = \"/kaggle/input/face-expression-recognition-dataset/images\"\noutput_base_path = \"data_split\"\n\n# Define label mappings\nlabels_map = {\n    \"angry\": \"no_confidence\",\n    \"disgust\": \"no_confidence\",\n    \"fear\": \"no_confidence\",\n    \"sad\": \"no_confidence\",\n    \"happy\": \"confidence\",\n    \"neutral\": \"confidence\",\n    \"surprise\": \"confidence\"\n}\n\n# Create output directories\ndef create_output_dirs():\n    splits = [\"train\", \"val\", \"test\"]\n    categories = {\n        \"confidence\": [\"happy\", \"neutral\", \"surprise\"],\n        \"no_confidence\": [\"angry\", \"disgust\", \"fear\", \"sad\"]\n    }\n    \n    for split in splits:\n        for category, emotions in categories.items():\n            for emotion in emotions:\n                os.makedirs(os.path.join(output_base_path, split, category, emotion), exist_ok=True)\n    print(\"Output directories created.\")\n\n# Load images and labels\ndef load_images_and_labels():\n    image_paths = []\n    labels = []\n    emotion_labels = []\n    \n    splits = [\"train\", \"validation\"]\n    for split in splits:\n        split_path = os.path.join(input_base_path, split)\n        if not os.path.exists(split_path):\n            print(f\"Warning: Directory {split_path} does not exist.\")\n            continue\n            \n        for label_name in labels_map.keys():\n            folder = os.path.join(split_path, label_name)\n            if not os.path.exists(folder):\n                print(f\"Warning: Directory {folder} does not exist.\")\n                continue\n                \n            print(f\"Processing {split}/{label_name}...\")\n            img_count = 0\n            for img_name in os.listdir(folder):\n                img_path = os.path.join(folder, img_name)\n                if os.path.isfile(img_path) and img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    image_paths.append(img_path)\n                    labels.append(labels_map[label_name])\n                    emotion_labels.append(label_name)\n                    img_count += 1\n                else:\n                    print(f\"Warning: Skipping {img_path} (not a valid image file)\")\n            print(f\"Loaded {img_count} images from {split}/{label_name}\")\n    \n    return image_paths, labels, emotion_labels\n\n# Copy images to the new structure\ndef copy_images(image_paths, labels, emotion_labels, split_name):\n    for img_path, category, emotion in zip(image_paths, labels, emotion_labels):\n        dest_path = os.path.join(output_base_path, split_name, category, emotion, os.path.basename(img_path))\n        try:\n            shutil.copy2(img_path, dest_path)\n        except Exception as e:\n            print(f\"Error copying {img_path} to {dest_path}: {e}\")\n\n# Main execution\ndef main():\n    # Create output directories\n    create_output_dirs()\n    \n    # Load images and labels\n    image_paths, labels, emotion_labels = load_images_and_labels()\n    \n    if not image_paths:\n        raise ValueError(\"No images were loaded. Check the input dataset path and structure.\")\n    \n    print(f\"Total images loaded: {len(image_paths)}\")\n    \n    # Perform stratified split\n    # First split: 80% (train + val), 20% test\n    X_temp, X_test, y_temp, y_test, emo_temp, emo_test = train_test_split(\n        image_paths, labels, emotion_labels, test_size=0.15, random_state=42, stratify=labels\n    )\n    \n    # Second split: 70/80 (train), 10/80 (val) of temp\n    X_train, X_val, y_train, y_val, emo_train, emo_val = train_test_split(\n        X_temp, y_temp, emo_temp, test_size=0.1765, random_state=42, stratify=y_temp\n    )  # 0.1765 ≈ 15/(100-15) to get 15% val of total\n    \n    # Print split sizes\n    print(f\"Train samples: {len(X_train)}\")\n    print(f\"Validation samples: {len(X_val)}\")\n    print(f\"Test samples: {len(X_test)}\")\n    \n    # Copy images to respective directories\n    print(\"Copying images to train...\")\n    copy_images(X_train, y_train, emo_train, \"train\")\n    print(\"Copying images to val...\")\n    copy_images(X_val, y_val, emo_val, \"val\")\n    print(\"Copying images to test...\")\n    copy_images(X_test, y_test, emo_test, \"test\")\n    \n    print(\"Dataset split and organization completed successfully.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T12:24:06.949779Z","iopub.execute_input":"2025-08-11T12:24:06.949943Z","iopub.status.idle":"2025-08-11T12:28:08.901833Z","shell.execute_reply.started":"2025-08-11T12:24:06.949928Z","shell.execute_reply":"2025-08-11T12:28:08.901156Z"}},"outputs":[{"name":"stdout","text":"Output directories created.\nProcessing train/angry...\nLoaded 3993 images from train/angry\nProcessing train/disgust...\nLoaded 436 images from train/disgust\nProcessing train/fear...\nLoaded 4103 images from train/fear\nProcessing train/sad...\nLoaded 4938 images from train/sad\nProcessing train/happy...\nLoaded 7164 images from train/happy\nProcessing train/neutral...\nLoaded 4982 images from train/neutral\nProcessing train/surprise...\nLoaded 3205 images from train/surprise\nProcessing validation/angry...\nLoaded 960 images from validation/angry\nProcessing validation/disgust...\nLoaded 111 images from validation/disgust\nProcessing validation/fear...\nLoaded 1018 images from validation/fear\nProcessing validation/sad...\nLoaded 1139 images from validation/sad\nProcessing validation/happy...\nLoaded 1825 images from validation/happy\nProcessing validation/neutral...\nLoaded 1216 images from validation/neutral\nProcessing validation/surprise...\nLoaded 797 images from validation/surprise\nTotal images loaded: 35887\nTrain samples: 25119\nValidation samples: 5384\nTest samples: 5384\nCopying images to train...\nCopying images to val...\nCopying images to test...\nDataset split and organization completed successfully.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!ls /kaggle/working/data_split\n!ls /kaggle/working/data_split/train/confidence\n!ls /kaggle/working/data_split/train/no_confidence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T12:28:25.164553Z","iopub.execute_input":"2025-08-11T12:28:25.164834Z","iopub.status.idle":"2025-08-11T12:28:25.518576Z","shell.execute_reply.started":"2025-08-11T12:28:25.164806Z","shell.execute_reply":"2025-08-11T12:28:25.517851Z"}},"outputs":[{"name":"stdout","text":"test  train  val\nhappy  neutral\tsurprise\nangry  disgust\tfear  sad\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppress TensorFlow warnings\nimport tensorflow as tf\ntf.get_logger().setLevel(\"ERROR\")  # Suppress additional warnings\nimport cv2\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\n\n# Define paths\ndata_path = \"/kaggle/working/data_split\"\n\n# Step 1: Load dataset\ndef load_dataset():\n    images = []\n    labels = []\n    categories = [\"confidence\", \"no_confidence\"]\n    splits = [\"train\", \"val\"]\n    \n    for split in splits:\n        for category_idx, category in enumerate(categories):\n            for emotion in os.listdir(os.path.join(data_path, split, category)):\n                folder = os.path.join(data_path, split, category, emotion)\n                if not os.path.isdir(folder):\n                    continue\n                print(f\"Processing {split}/{category}/{emotion}...\")\n                img_count = 0\n                for img_name in os.listdir(folder):\n                    img_path = os.path.join(folder, img_name)\n                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n                    if img is not None:\n                        img = cv2.resize(img, (48, 48))\n                        images.append(img)\n                        labels.append(category_idx)\n                        img_count += 1\n                    else:\n                        print(f\"Failed to load {img_path}\")\n                print(f\"Loaded {img_count} images\")\n    \n    images = np.array(images) / 255.0\n    images = np.expand_dims(images, -1)\n    labels = np.array(labels)\n    \n    if len(images) == 0:\n        raise ValueError(\"No images loaded. Check dataset path.\")\n    \n    print(f\"Total images: {len(images)}\")\n    return images, labels\n\n# Step 2: Train model\ndef train_model(images, labels):\n    X_train, X_temp, y_train, y_temp = train_test_split(\n        images, labels, test_size=0.3, random_state=42, stratify=labels\n    )\n    X_val, X_test, y_val, y_test = train_test_split(\n        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n    )\n    \n    y_train = to_categorical(y_train, num_classes=2)\n    y_val = to_categorical(y_val, num_classes=2)\n    y_test = to_categorical(y_test, num_classes=2)\n    \n    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n    \n    model = Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n        MaxPooling2D(2, 2),\n        Conv2D(64, (3, 3), activation='relu'),\n        MaxPooling2D(2, 2),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(2, activation='softmax')\n    ])\n    \n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=10,\n        batch_size=32,\n        verbose=1\n    )\n    \n    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n    print(f\"Test Accuracy: {acc*100:.2f}%\")\n    \n    model.save(\"/kaggle/working/emotion_model.h5\")\n    print(\"Model saved at /kaggle/working/emotion_model.h5\")\n    return model\n\n# Main\ndef main():\n    print(\"Loading dataset...\")\n    images, labels = load_dataset()\n    print(\"Training model...\")\n    train_model(images, labels)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T12:28:27.040527Z","iopub.execute_input":"2025-08-11T12:28:27.040831Z","iopub.status.idle":"2025-08-11T12:29:14.888687Z","shell.execute_reply.started":"2025-08-11T12:28:27.040802Z","shell.execute_reply":"2025-08-11T12:29:14.888013Z"}},"outputs":[{"name":"stderr","text":"2025-08-11 12:28:28.685860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754915308.870209      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754915308.921274      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading dataset...\nProcessing train/confidence/happy...\nLoaded 6313 images\nProcessing train/confidence/surprise...\nLoaded 2817 images\nProcessing train/confidence/neutral...\nLoaded 4301 images\nProcessing train/no_confidence/fear...\nLoaded 3613 images\nProcessing train/no_confidence/sad...\nLoaded 4220 images\nProcessing train/no_confidence/angry...\nLoaded 3487 images\nProcessing train/no_confidence/disgust...\nLoaded 368 images\nProcessing val/confidence/happy...\nLoaded 1327 images\nProcessing val/confidence/surprise...\nLoaded 598 images\nProcessing val/confidence/neutral...\nLoaded 954 images\nProcessing val/no_confidence/fear...\nLoaded 749 images\nProcessing val/no_confidence/sad...\nLoaded 937 images\nProcessing val/no_confidence/angry...\nLoaded 725 images\nProcessing val/no_confidence/disgust...\nLoaded 94 images\nTotal images: 30503\nTraining model...\nTrain: 21352, Val: 4575, Test: 4576\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\nI0000 00:00:1754915322.782537      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1754915326.729086     101 service.cc:148] XLA service 0x791e0400aeb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1754915326.729767     101 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1754915327.000008     101 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 53/668\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5226 - loss: 0.6981","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754915329.032691     101 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m668/668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.5575 - loss: 0.6831 - val_accuracy: 0.6536 - val_loss: 0.6297\nEpoch 2/10\n\u001b[1m668/668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6389 - loss: 0.6341 - val_accuracy: 0.6835 - val_loss: 0.5836\nEpoch 3/10\n\u001b[1m668/668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6906 - loss: 0.5815 - val_accuracy: 0.7121 - val_loss: 0.5566\nEpoch 4/10\n\u001b[1m668/668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7160 - loss: 0.5466 - val_accuracy: 0.7222 - val_loss: 0.5361\nEpoch 5/10\n\u001b[1m668/668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7425 - loss: 0.5113 - val_accuracy: 0.7296 - val_loss: 0.5263\nEpoch 6/10\n\u001b[1m668/668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7583 - loss: 0.4864 - val_accuracy: 0.7421 - val_loss: 0.5143\nEpoch 7/10\n\u001b[1m668/668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7769 - loss: 0.4537 - val_accuracy: 0.7412 - val_loss: 0.5172\nEpoch 8/10\n\u001b[1m668/668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7987 - loss: 0.4218 - val_accuracy: 0.7430 - val_loss: 0.5231\nEpoch 9/10\n\u001b[1m668/668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8179 - loss: 0.3921 - val_accuracy: 0.7390 - val_loss: 0.5263\nEpoch 10/10\n\u001b[1m668/668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8388 - loss: 0.3600 - val_accuracy: 0.7445 - val_loss: 0.5557\nTest Accuracy: 72.73%\nModel saved at /kaggle/working/emotion_model.h5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport tensorflow as tf\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\ntf.get_logger().setLevel(\"ERROR\")\n\n# Paths\nmodel_path = \"/kaggle/working/emotion_model.h5\"\nvideo_path = \"/kaggle/input/muzammil/muzammil.mp4\"  # Apna video path daal\noutput_video_path = \"/kaggle/working/output_video.avi\"\nhaar_cascade_path = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n\n# Load model\nmodel = tf.keras.models.load_model(model_path)\n\n# Analyze video\ndef analyze_video(video_path, model, output_path):\n    face_cascade = cv2.CascadeClassifier(haar_cascade_path)\n    if face_cascade.empty():\n        raise ValueError(\"Error loading Haar Cascade.\")\n    \n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        raise ValueError(f\"Video not found: {video_path}\")\n    \n    predictions = []\n    processed_frames = []\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n        \n        for (x, y, w, h) in faces:\n            face = gray[y:y+h, x:x+w]\n            face = cv2.resize(face, (48, 48))\n            face = face / 255.0\n            face = np.expand_dims(face, axis=(0, -1))\n            pred = model.predict(face, verbose=0)\n            predictions.append(pred[0])\n            \n            label = \"Confidence\" if pred[0][0] > pred[0][1] else \"No Confidence\"\n            confidence = pred[0][0] * 100 if pred[0][0] > pred[0][1] else pred[0][1] * 100\n            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n            cv2.putText(frame, f\"{label}: {confidence:.2f}%\", (x, y-10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n        \n        processed_frames.append(frame)\n    \n    cap.release()\n    \n    # Save output video\n    if processed_frames:\n        height, width = processed_frames[0].shape[:2]\n        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n        out = cv2.VideoWriter(output_path, fourcc, 20.0, (width, height))\n        for frame in processed_frames:\n            out.write(frame)\n        out.release()\n    \n    # Calculate percentages\n    if predictions:\n        predictions = np.array(predictions)\n        confidence_percent = np.mean(predictions[:, 0]) * 100\n        no_confidence_percent = np.mean(predictions[:, 1]) * 100\n        print(f\"Confidence: {confidence_percent:.2f}%\")\n        print(f\"No Confidence: {no_confidence_percent:.2f}%\")\n    else:\n        print(\"No faces detected.\")\n    \n    return confidence_percent, no_confidence_percent\n\n# Run analysis\nprint(\"Analyzing video...\")\nconfidence_percent, no_confidence_percent = analyze_video(video_path, model, output_video_path)\nprint(f\"Final Results: Confidence: {confidence_percent:.2f}%, No Confidence: {no_confidence_percent:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T12:29:19.044988Z","iopub.execute_input":"2025-08-11T12:29:19.045844Z","iopub.status.idle":"2025-08-11T12:53:19.663246Z","shell.execute_reply.started":"2025-08-11T12:29:19.045813Z","shell.execute_reply":"2025-08-11T12:53:19.662466Z"}},"outputs":[{"name":"stdout","text":"Analyzing video...\nConfidence: 49.98%\nNo Confidence: 50.02%\nFinal Results: Confidence: 49.98%, No Confidence: 50.02%\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"2","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}